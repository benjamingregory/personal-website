### One Big Takeaway from GDC 2025

Last modified: 2025-03-26

<br />

---

<div style={{ display: "flex", justifyContent: "center" }}>
  <blockquote class="twitter-tweet">
    <p lang="en" dir="ltr">
      You know what the biggest problem with pushing all-things-AI is? Wrong
      direction.
      <br />I want AI to do my laundry and dishes so that I can do art and writing,
      not for AI to do my art and writing so that I can do my laundry and dishes.
    </p>
    &mdash; Joanna Maciejewska (MoSS is on preorder now!) (@AuthorJMac)
    <a href="https://twitter.com/AuthorJMac/status/1773679197631701238?ref_src=twsrc%5Etfw">
      March 29, 2024
    </a>
  </blockquote>
</div>
<script
  async
  src="https://platform.twitter.com/widgets.js"
  charset="utf-8"
></script>

<br />

There's an understandable tension between creative tools and automation. There
is a legitimate fear that AI flattens creativity, making everything look the same.
This fear is not without merit but it can also cause industries like gaming to be
late adopters to tech that could otherwise help solve widespread problems, such as the overreliance
of crunch (i.e. unpaid overtime), a problem so bad [there's even a Wikipedia article about it](<https://en.wikipedia.org/wiki/Crunch_(video_games)>).

<br />
There has to be a solution and to see what companies are developing in this space,
I decided to attend GDC 2025 last week. Several companies are leveraging AI in game
development in exciting ways but there’s one I really wanted to highlight.

<br />
### Coplay: The Cursor for Game Development

Coplay has developed a chat interface plugin for game engines (currently supporting Unity,
with plans to expand to other platforms). At GDC, [Jos van der Westhuizen](https://www.linkedin.com/in/josvdw/),
co-founder and CEO, demonstrated how Coplay can make game edits and create entire new
levels from scratch, complete with physics and combat mechanics.

Rather than try to explain it myself, I'll let them do it:

<br />
[![alt text](https://img.youtube.com/vi/ipcNw4CVGiM/0.jpg)](https://www.youtube.com/watch?v=ipcNw4CVGiM)
<br />
After their presentation, I had the opportunity to discuss the product's future with
[Joned Sarwar](https://www.linkedin.com/in/joned-sarwar-59055827/), co-founder and
CTO. Our conversation sparked some interesting ideas about where this technology
could evolve.

<br />

_Note: The following represents my own vision for the technology's potential, not
necessarily Coplay's roadmap. They may disagree with all this entirely._

<br />
### Level 1 – Vibe Game Development

The “vibe” moniker can be polarizing; people kind of have a kneejerk reaction to it as either a revolutionary
accelerant or a tech debt nightmare waiting to happen. I think the truth is in
the middle. For both early learning and rapid prototyping, the ability to stay in
the flow, I do have to say, is game changing and Jos showed a great demo
where he showed this in practice.

<br />
Creating boilerplate menus, tweaking settings without having to dig through five
levels of customization options, adding scripting logic for character interactions,
and even physics to collisions and movement -- all of this stuff is trivial to do
with Coplay. This is the most immediate use case for Coplay and I think it’s going
to be a hit with both new developers who have a fuzzy vision of what they want but
don't necessarily know how to get there and experienced developers who know exactly
what they want and don’t want to spend time clicking through menus to get there.

<br />
### Level 2 - Agentic Game Development

From their experience building games, the team at Coplay clearly knows that devs
have workflows that are specific to their project and style. That's why there's a
recording button where you can have Coplay watch what you're doing and record the actions
to learn what is happening and what is trying to be accomplished. Walking the model
through each step of the process is exactly what you would do with a new member of
your team and will help the AI understand the context
of what you're asking and avoid hallucinating and creating something totally far afield.
This is where I think Coplay could really shine. Imagine a world where you have
a team of agents who are trained on accomplishing specific tasks.

<br />
Agents could specialize in scene development (e.g. Create a shader for a rainy day
scene with reflective puddles and dark clouds), asset creation (e.g. Generate a dense,
overgrown forest with varied terrain, ancient ruins, and soft, diffused lighting.),
or even NPC mechanics (e.g.Generate an AI script for an enemy that patrols an area,
chases the player on sight, and retreats when health is low.) Not only that, but
they could do it by following the same workflow you are already using. What’s more,
agents could leverage MCPs to plug into other generative platforms -- something Coplay
can already do with integrations into Meshy and Tripo for asset creation. Once AIs
are intialized with proper context, they could work together to take the seed of
a game that is tightly tuned and build out new levels, variations on existing levels,
or even entirely new games in the same style.
<br />

### Level 3 -- Abstracted Game Development

Watching Coplay go through the Unity platform, understand developer workflows,
and adjust settings on it's own is really exciting and demonstrates how ripe the game
industry is for AI disruption. But you can already see a world where Coplay lives
as a full abstraction layer on top of any game engine. In this world, Coplay is the
main interface with the engine living underneath it. Users interact with Coplay
similar to how Roblox has done it with a simplified workflow on top of their own game engine.
In this version, however, the game engine becomes secondary to the translation
layer (in this case Coplay) and game development as a feature can become as
easy as embedding a chat or drag-and-drop interface with an iframe representing the Scene / Game View.

<br />
This may sound kinda far-fetched but there are two analogues that come to mind here.
<br />

#### 1. Model Abstraction in AI Feature Development

We can easily forget this talking with other AI creators but for the most part,
the average person (even one using AI tooling) does not know which models they are
using and in what combination. Don't believe me? Ask someone not in tech what their opinion
of GPT-4.5 vs Claude 3.7 Sonnet. (I already know some of you reading this already have strong opinions
ready to go.) But, to an average person, when trying to get through customer service, ask Siri
a question, or edit a document, they just care about results. The best companies
creating workflows for AI development (make.com, n8n, langchain) are really good at abstracting
away implementation details so the user can focus on results. In a similar way, Coplay
could make reasonable defaults for settings / parameters inside the game engine and
allow developers to focus on the game they want to create. What's more, Coplay could
offer a set of primitives that encapsulate Unity workflows / logic that can then be used
by other companies to develop their own Roblox-like platforms.

<br />
#### 2. Native Code Abstraction in React Native

React Native enables cross-platform development through a write-once, deploy-everywhere framework.
React Native is not so abstracted as to be a drag and drop interface; in fact, it is extremely powerful
and can be complex in the types of apps and structures you want to create with it.
When a specific native feature is required, developers can "eject" their application
into platform-specific code (e.g. Java / Obj-C). This is a one-way ejection so there’s
no going back but then you have a full project of native code set up with a lot of
boilerplate already done. A game engine itself is a set of abstractions collected
into a single environment but that's not to say that it’s the final form of game
development. Unity / Unreal (the native source) may be the right tool for the job
eventually for pixel-perfect clarity but not for prototyping and quick iteration.

<br />

### Remaining Thoughts

With the high-level vision out of the way, I wanted to dive into some of the technical
challenges that Coplay will need to overcome to reach these levels of abstraction. I
think there are two broad areas that Coplay will need to focus on to succeed.

<br />
#### Intelligent Context Management

Modern games comprise thousands
or even tens of thousands of unique files—a labyrinth of assets, scripts, and configurations
that could potentially be prohibitively expensive (either in pure $$$ or just time / efficiency)
to tokenize and process in their entirety. This is perhaps Coplay's most significant technical
challenge but could also be a competitive advantage if done correctly.
The true breakthrough for an AI implementation of this scale isn't merely getting an AI to
perform impressive tasks. It needs to be able to parse an enormous context yet still operate in
near real-time to feel like a true extension of the developer. And while it doesn't need to be cheap
necessarily, it does need to be able to justify it's own existence so throwing more resources at it
isn't the solution if the cost becomes prohibitive.

<br />
Advanced techniques might include:

<br />
_-Persistent context on high-level project architecture_

_-Graph-based representation of project components that maps relationships_

_-Dynamic context windowing that focuses on relevant files and relationships_

_-Specialized agents trained for relevant context retrieval_

<br />
#### Autonomous Agent Governance

Coplay comes with variable settings for the extent to which the AI should make changes to your project (e.g.
Normal vs. Beast mode where the AI will take on much more responsibility and independent judgements.)
The question becomes then that if you have a team of agents working on a project, perhaps completely
independently, how do you ensure changes made by these agents are in line with the vision of the project?
For that, I think there are three necessary features (which, by the way, they may already be working on, not sure):

<br />
**1. Version Control** - This is a must-have for any project but especially for one
where multiple agents are making changes. There already is [Unity Version Control](https://unity.com/solutions/version-control)
but imagine being able to see the agent's [chain-of-thought](https://arxiv.org/abs/2201.11903),
identify where an agent went off-course, roll back to that previous state and start
a new iteration. Or branch it from a certain point and test an alternate version.
With this version control for generative AI, agents can create as many permutations
of a project as desired and the developer can choose the one that best fits their
vision.

<br />
AI-specific versioning would incorporate:

<br />

- The agent's chain-of-thought reasoning for each significant change
- Decision points where development could have followed multiple paths
- The ability to branch from specific decision points to explore alternative implementations
- Metadata about contextual factors influencing decisions

<br />

**2. Agentic Permissions** - Like any external developer that you would grant system access to,
there need to be guardrails of what resources the AI can access and what it can do with them.
The Normal vs. Beast mode is the beginning of this and helps match developer velocity with risk
tolerance. But that assumes that there is a [Human in the loop](https://en.wikipedia.org/wiki/Human-in-the-loop),
which is a massive help in ensuring each change is in line with the project vision. In a fully agentic system,
being able to to give specific permissions and motivations to agents helps simulate a real-world team environment.
You can easily imagine a system where one agent responsible for asset creation is given more leeway to
experiment with new styles and techniques while another agent responsible for combat mechanics is
given more strict guidelines to follow.

<br />A sophisticated permission system might:

<br />

- Assign specific capabilities and restrictions to different agent types
- Create domain-specific permissions (e.g., visual assets versus gameplay mechanics)
- Implement graduated autonomy that increases as agents demonstrate reliability
- Establish approval workflows for changes exceeding certain impact thresholds

<br />

**3. Contextual Audit Logs** - Seeing that a setting was modified is one thing but understanding
the context of why it was changed, what logic went into making that change, and what else was changed
alongside it is essential for ensuring the project doesn't become a black box that no one can understand.
I believe this was mentioned in the talk but I think it's worth reiterating that this is a critical
feature for any AI-assisted development tool.

<br />

A comprehensive audit system would capture:

<br />

- The contextual information available to agents when making decisions
- The reasoning process leading to specific implementation choices
- Relationships between seemingly unrelated changes
- Potential alternative approaches considered but not implemented

<br />

---

<br />

There's a lot more to say about Coplay but I'm out of time for now. I think, in summmary,
the most promising aspect of Coplay's approach is that it doesn't attempt to replace
human creativity. Rather, it amplifies by removing barriers between conception, experimentation,
and implementation. In doing so, Coplay might actuallly help fulfill the original
promise of game engines themselves: making game development more accessible without
sacrificing depth or possibility.
